{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "<a href=\"https://colab.research.google.com/drive/1S153LgzXKIjiA3QjBrEB5h_dxztSPf4o?usp=sharing\" target=\"_blank\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/>\n",
    "</a>\n",
    "\n"
   ],
   "metadata": {
    "id": "rOijBXWwjmzs"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U61uVjxRivkq",
    "outputId": "76ab687e-50ec-43d1-bddf-a5dd5310d5c8"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m46.4/46.4 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m193.2/193.2 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m117.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m94.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m95.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m67.9/67.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m141.3/141.3 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m413.7/413.7 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m128.9/128.9 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m43.4/43.4 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m93.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-metadata 1.17.1 requires protobuf<6.0.0,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\n",
      "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\n",
      "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#1 Install libraries (if needed)\n",
    "!pip install -q unsloth \"torch>=2.1\" transformers peft datasets trl accelerate py7zr"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Main imports\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer"
   ],
   "metadata": {
    "id": "NnCHNMAYki93"
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#2 Load TinyLlama-1.1B-Chat\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    max_seq_length=4096,\n",
    "    dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=True,\n",
    "    random_state=42,\n",
    ")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "utilcyJajmVk",
    "outputId": "f19d5679-b1e0-488b-881d-5dfa4d5b99e3"
   },
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==((====))==  Unsloth 2025.4.1: Fast Llama patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Unsloth: unsloth/tinyllama-chat-bnb-4bit can only handle sequence lengths of at most 2048.\n",
      "But with kaiokendev's RoPE scaling of 2.0, it can be magically be extended to 4096!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#3 Load small IMDb subset (classification)\n",
    "from datasets import load_dataset\n",
    "\n",
    "imdb_dataset = load_dataset(\"imdb\", split=\"train[:2000]\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 325,
     "referenced_widgets": [
      "1438f6e3568f4b12baebecbde517a9b4",
      "877942e214884eeebcd8fda0633b45cd",
      "d9e34371e17f4f7fbca95069e5f9dab6",
      "2cc0394c60a045e5a4aa382560aebc78",
      "1064bf1525a542d48f7c9d71d201e8da",
      "1fd721958925405f93f4fa018d978743",
      "c8b0a4af62ee444b9b6d4ac57b7df687",
      "d259f90b6c8f469d87edc97f1ffa37f6",
      "bdfa55f4bd7a4acbbf8950447a397959",
      "2af5c48093924badb53657cb5d1d6d3b",
      "b21741a386f64d99b1e659d79755406f",
      "3e11f06b350845c8b398911b75fd5826",
      "079f83c95af342a3829ca24d2a8f3d46",
      "f8e5d19e8f494f34a25041ae51150b38",
      "2e60acb2606749029474c8ea48a699bc",
      "1a7f51727457467985cf0ad2b7a07ec9",
      "9ecd4b2e33ca436cb89be368e786b0f7",
      "980ba31c39a143a297233d023e4b40b6",
      "72d1b88e480942109b4acbebf7664509",
      "51c5d26f7bba4320a008aa7a462220fa",
      "143d174fb4ab4c0aac4593604a9016f0",
      "1c24cf6ffe7842cbbf8dc7028d0dfb10",
      "4752c04656184a8c9b9554445cec966f",
      "beec920420de4cd99a567f4b60839dd0",
      "751a6f8f8a8d43bb9f4b0e0172b5b5cc",
      "d13e24cb6199449397d06b57e3042779",
      "df464c4dc1d84170b5a9dcde53cd08aa",
      "d99c3149d727412ea29ba468337ba5b8",
      "b6bf52767d064b55be2487b13be2e933",
      "bb41e7ee8c204586aeb0efdcdbc6127b",
      "69602fab2c954f8f957444ed40f916b1",
      "5852c78ce5e0435595df9ded0e22f061",
      "812a6139cad24aeda0ed4f7322ecc390",
      "e1f49bb40e2a44deba13107c470ce778",
      "4b6d96d0ea7f421f85a3a18ad46e165a",
      "1a70646c3a944278af4c851e639b9516",
      "6836d9e8ba9f480a806ee191dc30cdbb",
      "c904613bb2ff4f08be270b5ef4819f5f",
      "3a726239887244daa82beb57ae126443",
      "b55505721d4c4f6dacea9a0d0e6ff4b0",
      "18076360f82b433aa96b75eb4878f967",
      "c4c63e3f62b844ed80276b8c664ef227",
      "b8cda513202548fea40c546c34641e23",
      "95c7bae482fe4b258da87ac52eefd46a",
      "6f62185e9a7e44659d3263306db5de04",
      "c710ec8f70d6419f92424cd46c39a181",
      "08d986c35c904bb8b3a83ccc86c7fbd7",
      "17472ac45ac243999d878553fcc6f2b8",
      "76742299d6694342b2bab2d258f35d78",
      "6b455a5865c14ed6a42e29d4135bc8e6",
      "edb37de320fa455f80c9b35ce6605049",
      "313693071097468a8ac2373e04bed8e1",
      "d2e771dae1014765a0e65284c198c7ee",
      "189b520db5534aa1ba146c28ab915410",
      "b3a908e623de4717b7f23703b7815397",
      "dd404e9d898e453197609fe8feb03ea4",
      "17468a41df3649f69d6228f9bc83e236",
      "71b7d8899e884baaaf571cdd14390495",
      "15e0f54d440c43a986adcb5283cbc3a8",
      "8add5357646f4d518b001290ca6b89b4",
      "7dbe049faece49f39873d1aaa452d721",
      "4e0fc4a1109c44548c9ada1dd769b4db",
      "dd43bb636cce4b3087e747f6637ed658",
      "4e03ee5b976d4c71b59c274e1490032e",
      "32d2daf2a99a4aacb4fdd4abf5871d8e",
      "b4e914b335fe4cefb237251e348d493b",
      "5d3a4234f2104d3da2696d7cdb0f35a6",
      "d1f194b82b52476e9e939e5383a26f4e",
      "fad397ed0ad0413ba809be393f5fe8ea",
      "11ebe6ffc220469fa48f84eb016fdf25",
      "ff4b38a41eaa4d3bae727c4360f65b6d",
      "b4d2d75cc71a47779d7d4048eb20f3dc",
      "75f8574e7cdf4d749c25184486f18639",
      "b14313d211d34f869f7db91c47991716",
      "9829778718a64942a45ec33b63814635",
      "01f1260dfbf54c82b8a70d3ef1966aca",
      "8192098e1e174b58a308d5b2708dfcfe"
     ]
    },
    "id": "NtiR8iSWjmYM",
    "outputId": "e36eb55a-82d7-4e07-f9f8-633a4e857407"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "README.md:   0%|          | 0.00/7.81k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1438f6e3568f4b12baebecbde517a9b4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3e11f06b350845c8b398911b75fd5826"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4752c04656184a8c9b9554445cec966f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "unsupervised-00000-of-00001.parquet:   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e1f49bb40e2a44deba13107c470ce778"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6f62185e9a7e44659d3263306db5de04"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dd404e9d898e453197609fe8feb03ea4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5d3a4234f2104d3da2696d7cdb0f35a6"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#4 Load samsum dataset (small subset)\n",
    "samsum_dataset = load_dataset(\"samsum\", split=\"train[:1000]\", trust_remote_code=True)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209,
     "referenced_widgets": [
      "174d4268789e4e678c01155b9b5f6721",
      "93af0c2bc40d422baffa3e0f4e58cc42",
      "c250e5bd032c46c490a38a8d2fad2a5e",
      "3143e93189524879b96b1f58dae2a950",
      "04170d9127644b5698d61601a9d71f0a",
      "b6a81ae7466b41818f4755a4af31de7a",
      "9ab5accff39f4666ad7e381c436fa6b5",
      "779a2fb1133c42b6a5930a6f4c97bef4",
      "255396e1e20245b4b345925afa5cb958",
      "e78ed5f1fe814ddd8c8310704588baf8",
      "d1729be3b0594ea58b4cdb57ec049113",
      "3793d79a7c9e47fe9767547a6fbb8455",
      "9ecf845f9aee4f9faf4c0a2504839073",
      "f66ec72852054fbdae663fa056b7f77e",
      "e5279ae1fdc6424f99731c623717ccfc",
      "7d104d480fa54daf94a06579fc2e5240",
      "04386d30fced4157900b1d9e65d888d9",
      "32f264cd717d4e2a8094301d8d9ff730",
      "1b319dc4cff24cad8ea5d89ea0333067",
      "c61998547a1b4174b4e6c1fcb524d841",
      "f981e1c2234f487bade1a6161a25395c",
      "824673f0badd4c3a9717204105e8ad05",
      "68118e8ea40c486c9151f3abcbc8f559",
      "ef8e910ce0c7465191f0bf64109020ae",
      "f119b70a75f1454db9c0f8d8163a4636",
      "683a50e68c9145249f4b626fc124cf95",
      "311637f5cc894841ab0302243d85fa1a",
      "82a2bce1f20c4aa3975a1432d5a2c12e",
      "6fab41e4b22641cc8a1cd54c8eee348c",
      "aea7a0f3cfe944328c9d6b9f4e205ed2",
      "ac45d0f6ed5245f58b2c8d1ee902005d",
      "9dbe814f8a504f3e8c080b9a3b3ee902",
      "d39cc352fb44477c8ec8b096a8daf10f",
      "d057abf0d8e045ca925c9a9c24f24450",
      "ff8bca7f6677430aa7c54a4f20b852ff",
      "21e606fd718f483da8eb002c68581879",
      "611dde2bad354a6abcd310dacb6f5551",
      "e149a62c0712486e94b47ca04dcc1a02",
      "ab5c6ba58e474a729f7d297769c0edd5",
      "33a283b59ddf40ac8aabc97fa59731c5",
      "561e3d8a2a7b451dadb62576d0b392f5",
      "2729e70530f54071b102a14a25949fd6",
      "8aef40096e8541cba1b8db230bb05f81",
      "122bbc57189f40858dc22a58ad0aebcb",
      "73b3f4a43b7c462d88c82e03eb4dadc9",
      "3704ea75856a47398d2f4d4f44cb8395",
      "1fd4efbd487c4a539dbfa3b02019fffa",
      "84424fb02a074985b500185765652c3b",
      "dd346366f5bc4e51bf879fd5476dfcb7",
      "8bb9831f5534427c9a11ac92864d0d68",
      "46262bee8bb645ed84b8b69b65037293",
      "398ebe7751f24a20bd66a1161e381130",
      "0ce19b4d3b514515ba1e5b417fd48958",
      "66518d5f066f447785824a0d6d185438",
      "b2e2f673970e4a52b167236a37f21dbd",
      "98194b70ea72467e82be9c6ca0172283",
      "a98ce2b2698442b8a75c0b0fb3f84bfd",
      "6123f1ff4e0748d383c196f1c1cc3e12",
      "6e8ca2a76d2d435cbfb1a116463c8683",
      "e02b17268b7f422d8b1d7ce49cc21ecb",
      "4e23e57c8e5a4fc29c55dc113777e99e",
      "1939c064aff94bb58f7d8d70dc397c5e",
      "9adb4096294c4fb5b22f13fc46447fe0",
      "920f169f770a4f648f10de0cfd393ab0",
      "831007ad97834c2f9d98c8db3ff055d3",
      "8a3f7fb483684ec8944bcd252f917823"
     ]
    },
    "id": "n96tK-fkjma7",
    "outputId": "76e3c5f8-e1df-4c30-9b2b-548d032f705a"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "README.md:   0%|          | 0.00/7.04k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "174d4268789e4e678c01155b9b5f6721"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "samsum.py:   0%|          | 0.00/3.36k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3793d79a7c9e47fe9767547a6fbb8455"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "corpus.7z:   0%|          | 0.00/2.94M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "68118e8ea40c486c9151f3abcbc8f559"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/14732 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d057abf0d8e045ca925c9a9c24f24450"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating test split:   0%|          | 0/819 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "73b3f4a43b7c462d88c82e03eb4dadc9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating validation split:   0%|          | 0/818 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "98194b70ea72467e82be9c6ca0172283"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#5 Format Classification Dataset (OpenChat template)\n",
    "\n",
    "def format_classification(example):\n",
    "    sentiment = \"Positive\" if example[\"label\"] == 1 else \"Negative\"\n",
    "    return {\n",
    "        \"text\": (\n",
    "            f\"<|im_start|>user\\n\"\n",
    "            f\"Classify the sentiment of this movie review:\\n\\n\\\"{example['text']}\\\"\\n\\n\"\n",
    "            f\"Respond only with Positive or Negative.\\n\"\n",
    "            f\"<|im_end|>\\n<|im_start|>assistant\\n{sentiment}\\n<|im_end|>\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "formatted_classification = imdb_dataset.map(format_classification)\n",
    "formatted_classification = formatted_classification.remove_columns([\"label\"])\n",
    "\n",
    "# Format Conversational Dataset (ChatML template)\n",
    "\n",
    "def format_conversation(example):\n",
    "    return {\n",
    "        \"text\": (\n",
    "            f\"<|im_start|>user\\n{example['dialogue']}\\n\"\n",
    "            f\"<|im_end|>\\n<|im_start|>assistant\\n{example['summary']}\\n<|im_end|>\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "formatted_conversation = samsum_dataset.map(format_conversation)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "4b0557bce44d47b292e3210b9dc66272",
      "38a8396f25114aea9e3a596025b0b031",
      "630b37c925ce427898f7555fc843d9b0",
      "39a1f37a992e4ce58658b97314d55003",
      "f69ae8b0932f49deb3c005e36d782666",
      "652e3f04d981415ca934cfa167efff1c",
      "c2b0a7b064e84171bbfdde7ffba05e7a",
      "9ff3f4cc050d4832bbc5509a498d1f53",
      "86440ad7bc9b47b89125a49169d221ae",
      "9d2806ba45564d8784e14d4af53ece00",
      "532a2e7682214be68ccdf8a5f478998f",
      "e187e37552ec49e5be024214973a69fa",
      "e64c0e3fa7dd44d4a64bb1e43d981aff",
      "8e086168da0b4e9b97f00181815a8665",
      "b144164744a1449e8fbb4fe189945e45",
      "1807e95c07b14cf0967be097f025f71f",
      "3b7e2a518c0a45ba9a133573a63ac07e",
      "20c5c94ce2c3409f8cf4f57109807a0c",
      "8f806c0caff64659a96d32e76778fa3f",
      "faa4ad08fa93435c8899284c28efec47",
      "68b75b19b663424cba551387218a8723",
      "9af4b47cc5e64601b08eb068517c29da"
     ]
    },
    "id": "ipecJP5Jj0-s",
    "outputId": "597b49d9-7fd8-4b0d-8215-416fe66317e1"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4b0557bce44d47b292e3210b9dc66272"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e187e37552ec49e5be024214973a69fa"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#6 Merge classification + conversation into a single dataset\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "multi_task_dataset = concatenate_datasets([formatted_classification, formatted_conversation])\n",
    "\n",
    "# Shuffle merged dataset\n",
    "multi_task_dataset = multi_task_dataset.shuffle(seed=42)\n",
    "\n",
    "# Preview\n",
    "print(multi_task_dataset[0])\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PmVLVO0Hj1Bj",
    "outputId": "80984774-787a-4f20-e6ba-769f6b142a35"
   },
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'text': \"<|im_start|>user\\nGemma: Forgot my keys :(\\r\\nSuze: Come by my office\\r\\nGemma: okay thanks!\\r\\nSuze: no problem\\n<|im_end|>\\n<|im_start|>assistant\\nGemma will come by Suze's office as she forgot her keys.\\n<|im_end|>\", 'id': '13728796', 'dialogue': 'Gemma: Forgot my keys :(\\r\\nSuze: Come by my office\\r\\nGemma: okay thanks!\\r\\nSuze: no problem', 'summary': \"Gemma will come by Suze's office as she forgot her keys.\"}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#7 Tokenize merged dataset\n",
    "def tokenize(example):\n",
    "    tokens = tokenizer(\n",
    "        example[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=4096, #Extended\n",
    "    )\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
    "    return tokens\n",
    "\n",
    "tokenized_dataset = multi_task_dataset.map(tokenize, remove_columns=[\"text\"])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "cc964eb9155642a7b93bfa139dc64b32",
      "5a53e87b2b054adbbb0a02efe34ce553",
      "1b17181e8638424c8f126c09d1b6109c",
      "e765a6f6dc7c41539697c2ef3f77063d",
      "e30b05e758474a9cbff33517a09a13cf",
      "cb74927a780f419387093bafbe9952d7",
      "3159ef8761684bd6984a900f69af79a8",
      "e2dfe6588cc740b497410e08e40fc130",
      "b9c0b92ff8af4ee489bd2fa20773b440",
      "ed08d676420845df978d651ddef6ec83",
      "37a8ffd26dd64b98bd58c3b38c10e373"
     ]
    },
    "id": "aVU12_J2j6Gv",
    "outputId": "7c94dcaf-003c-4a13-d785-3541f59b956b"
   },
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cc964eb9155642a7b93bfa139dc64b32"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#8 Fine-tune using SFTTrainer\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"tinyllama_multi_task\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=1,\n",
    "    max_steps=100,\n",
    "    logging_steps=5,\n",
    "    save_strategy=\"no\",\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=4096,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 848
    },
    "id": "naRqqZOij6JV",
    "outputId": "720ea9cf-e1f7-454e-aa5a-7ff7a4ac9020"
   },
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 3,000 | Num Epochs = 1 | Total steps = 100\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 2\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 2 x 1) = 4\n",
      " \"-____-\"     Trainable parameters = 4,505,600/4,000,000,000 (0.11% trained)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 12:43, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.574900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.447700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.463900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.362100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.428200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.403100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>2.273700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.297900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>2.306900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.208300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>2.298900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>2.294600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.146000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>2.257100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.155400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>2.178900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.203000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>2.141500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.142000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=2.2942074871063234, metrics={'train_runtime': 771.1896, 'train_samples_per_second': 0.519, 'train_steps_per_second': 0.13, 'total_flos': 1.02139623899136e+16, 'train_loss': 2.2942074871063234})"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#9 Save model and tokenizer\n",
    "model.save_pretrained(\"tinyllama_multi_task_model\")\n",
    "tokenizer.save_pretrained(\"tinyllama_multi_task_model\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SIOTTYykj6MQ",
    "outputId": "d681daee-e605-431f-8690-210afe4347dc"
   },
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('tinyllama_multi_task_model/tokenizer_config.json',\n",
       " 'tinyllama_multi_task_model/special_tokens_map.json',\n",
       " 'tinyllama_multi_task_model/tokenizer.model',\n",
       " 'tinyllama_multi_task_model/added_tokens.json',\n",
       " 'tinyllama_multi_task_model/tokenizer.json')"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#10 Inference Testing\n",
    "\n",
    "def multi_task_inference(prompt, max_tokens=100):\n",
    "    formatted_prompt = (\n",
    "        f\"<|im_start|>user\\n{prompt}\\n\"\n",
    "        f\"<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    )\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_tokens,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return decoded.split(\"<|im_start|>assistant\\n\")[-1].split(\"<|im_end|>\")[0].strip()\n",
    "\n",
    "# Test Classification Task\n",
    "print(multi_task_inference(\"Classify the sentiment of this review: 'The movie had breathtaking scenes and strong performances.'\"))\n",
    "\n",
    "# Test Conversation Task\n",
    "print(multi_task_inference(\"Hey, are you free this evening? Let's catch up over coffee!\"))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I4hDOJqKkBfx",
    "outputId": "c63331cd-3fcf-4b8f-8b01-7c05d85f3b8c"
   },
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Positive\n",
      "Sure, here are a few coffee shops in the area:\n",
      "1. Starbucks\n",
      "2. Dunkin' Donuts\n",
      "3. Coffee Bean & Tea Leaf\n"
     ]
    }
   ]
  }
 ]
}