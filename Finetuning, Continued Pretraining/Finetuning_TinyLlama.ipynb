{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<a href=\"https://colab.research.google.com/drive/1qWyYKcaXtqF0ewSxpY7fS0HkEZ6DdyZE?usp=sharing\" target=\"_blank\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/>\n",
    "</a>"
   ],
   "metadata": {
    "id": "kU0TDlDthah1"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RgkQW-xBUDPr"
   },
   "outputs": [],
   "source": [
    "!pip install -q unsloth \"torch>=2.1\" transformers accelerate peft datasets bitsandbytes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -q trl"
   ],
   "metadata": {
    "id": "79hH68PLW1ki"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v5l_XSh6UmJG",
    "outputId": "e4fadd2a-0119-4186-b333-97a155cef8cf"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\ud83e\udda5 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "Unsloth: Failed to patch SmolVLMForConditionalGeneration forward function.\n",
      "\ud83e\udda5 Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    max_seq_length=2048,\n",
    "    dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=True,\n",
    "    random_state=42,\n",
    "    use_rslora=False,\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365,
     "referenced_widgets": [
      "200a7f97c79e4c10b5b5538e815352fa",
      "0cfa1cf8c6564cbb8f40cb758a428d3f",
      "59213d746cfc412da456024be01f2802",
      "a420bfee5b3b45bfbfac0b76f89921df",
      "bb6f6effd21641f39bbe40355f810ba3",
      "bd439c578be94f66830424a3414d752c",
      "170c927a148443f7be7dbc71973fa437",
      "e3e056db498f4519adc8abe9a81af593",
      "401b4dc25b8e461099f65d251e014d67",
      "aaede6273a6544a89842f126e507c5cf",
      "6d1366ed4b7a4d9184f22ec0ddd8b06e",
      "6ceece0205454a6ea3e09221051eb5d6",
      "9cd730a289cf4438a27cdf72d3439144",
      "397c80d89f814110a3b047c71ec4cf00",
      "df9e9011998c4368a05a61e074d03e09",
      "43e4377fdd4f4ec79e5d8efa62197f25",
      "08a0bd797e074a19a9501c812bc79bbe",
      "76020576c59b4ed8b42d67385aa5442b",
      "76ee3671a1564535b63c7b1d5af12167",
      "5c828ceb8e8f411ba8f9c54a666801f5",
      "be382ef704fa41a49303a7bab4a6c037",
      "a173ea7465cf4ab4988c5fd8c915be3b",
      "f70658bce3114e45b214fba101ec5ad0",
      "909cff339af74dc9b02149280d1ea738",
      "24c2e1d88eac4247b6b362bdade93f1d",
      "d9c42f2620904512b37b33eb1444e8a2",
      "a8827f3526d9497ba7e2c07024b659a4",
      "2ca966c11c1f4de78d73d27e7c5dfccc",
      "8ddb742f662847ae9560315cc2419323",
      "f0a047bbb35d4eee9e6610a83ae4031e",
      "b7af26aff49c4ffeb772eb9c376dd080",
      "c01d8edf08544cd998ea9286a8750455",
      "28ee599c6ff44b4f960314fb28d41035",
      "8b245eb144b44800937c86cf81fdaccd",
      "6a9dec8530cd461a95e34baec922f5d1",
      "a9da4adb3f084cd6a504e57bdd76b509",
      "21d2234ff9e1434898216efef700783d",
      "36d4213d7db14995a3edd9629414eb2b",
      "db82f46f11674315af3f597782e70ef0",
      "b135641c03414dfab3a53e76e90c3728",
      "cbc40d22d9ac407095356a4f4aac5df2",
      "067157ed4fc14372b11ce211d75e144f",
      "3510bcc8979545ee96fce6b3ff55e50a",
      "b01a10dd2935494888a928acb73d6807",
      "abeb76bd6798477db9fe4c7952caa0dc",
      "4adbbd7c88284315aa3a1d847f271987",
      "3b5b99a13f0f4afd94572401dee5ca5f",
      "07b1ce23f5b544839015485e0283ad97",
      "e2c47c5f7dbd45918f3de1c21560d030",
      "670965473adf4f3e9f9dc35cbc549b2c",
      "f8ed583b3f024fcebafc75b2e5ba84c0",
      "ffeed492aa9b4e59aa2d72fe6ee3a247",
      "012b6d7654744667bbacc0d0d31fee1f",
      "8098afd2731f45df8442e89df40b8557",
      "ee24e6f179dc4d649db49dcafe4d6a2b",
      "018869e09e3849a39d96db729980ff14",
      "74fc92bfc5c34dbaba6876451ce2c8de",
      "2912b6b10bd54eeb9125c9184f94fc3d",
      "596f825d43844504b8ae2997eda8b536",
      "1ff2c20439cb4dafa71eb39f51a7f187",
      "43af0cd15d48478c8231db9c039086d6",
      "1cf9afc8b338495f83e5daa672c026b7",
      "c34cc4fb93594db290de9f5c60716d0a",
      "b759c434227b4d03905ac3078a77a2df",
      "f15b77ceb5ca4430beca8afaa841a2ed",
      "8454148d076e44d595ed6caa7597cc41"
     ]
    },
    "id": "LpqRF86jUOl6",
    "outputId": "8420ad44-3590-42fb-cc28-795fe8ca762c"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==((====))==  Unsloth 2025.4.1: Fast Llama patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/762M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "200a7f97c79e4c10b5b5538e815352fa"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6ceece0205454a6ea3e09221051eb5d6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.37k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f70658bce3114e45b214fba101ec5ad0"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8b245eb144b44800937c86cf81fdaccd"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/438 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "abeb76bd6798477db9fe4c7952caa0dc"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "018869e09e3849a39d96db729980ff14"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth 2025.4.1 patched 22 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train[:10%]\")  # Use 10% for quicker demo"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168,
     "referenced_widgets": [
      "5e4f7b9594b44d05bf854e364fd729a3",
      "a5e8db45c1174337a3c16697ad0e00b9",
      "f81250805eae449bb8dd769d081db70b",
      "0d41dee532ec4e08936e8b157b7fd2c0",
      "ed2e4d63f9854857896989ae9efbef71",
      "36bcdd77ab5f473293fa62b4be9f91c1",
      "53e456d47ac6447d801d6a0c4cd61f81",
      "09e7eb2b4c7c41bb933e8c23e8416d71",
      "3e067d3c48ba4814aed36e0c4b46064c",
      "c0a11bf37eb148df81874b4b361ff890",
      "5dac3931901c431eb0b3ccba7986394a",
      "501895d797b449488b8bbf763c95a5e8",
      "92394fe6f11346efb3c6aeabd05338ca",
      "0370de0930a84b0bb5a2fb8e0cef6828",
      "ed828ff5a0434f4a95db4654b39d755c",
      "d4d7858bff824eb88e68980ddd398bca",
      "accb6fdd9c3443b1bbcbdeae50900253",
      "0cb90c6084df4148b77f5be90ddddb5e",
      "62601bb9a12d4c7492bb873cabbb2836",
      "a4a57c53cd444c98977501b966abda0b",
      "ee2f166a7c7f4bb3812eddc492ea89ec",
      "92292757fbee495183d4adad843062b4",
      "5ab7debf8cb34fafac5c2ae6812afa56",
      "91a488ff4d1444de99e6d1b7256036cd",
      "f7ee647066a34adc865d194583645d54",
      "e059dd40493c4cc8ae1c6370833e49f8",
      "d55e06f886684aa8b87c4a9e26948446",
      "64238efc4c664aebad6629157fb39163",
      "7897e04dd2144d62910c3337575b46d6",
      "890aa18fe8ca492c99f9f202cf502d0c",
      "af68f87304c34984af2e8df230c90fa8",
      "ea74a40d2321435ab87b63e9d9d438b5",
      "dfc2555caa43475a82169093be63a857"
     ]
    },
    "id": "Qcq4Ra2WUFfc",
    "outputId": "3fbe89aa-c14c-4f6b-c0b8-b1f5c0830058"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "README.md:   0%|          | 0.00/7.47k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5e4f7b9594b44d05bf854e364fd729a3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "(\u2026)-00000-of-00001-a09b74b3ef9c3b56.parquet:   0%|          | 0.00/24.2M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "501895d797b449488b8bbf763c95a5e8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/52002 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5ab7debf8cb34fafac5c2ae6812afa56"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def format_chatml(example):\n",
    "    user_prompt = example['instruction']\n",
    "    if example.get('input'):\n",
    "        user_prompt += f\"\\n{example['input']}\"\n",
    "\n",
    "    return {\n",
    "        \"prompt\": f\"<|im_start|>user\\n{user_prompt}\\n<|im_end|>\\n<|im_start|>assistant\\n\",\n",
    "        \"output\": f\"{example['output']}<|im_end|>\"\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(format_chatml)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "7ea0c3bc1b1b46e1a2915aa98861b298",
      "6a1b5db87a7448969566fdc57c9e3493",
      "253d4f3f3ad0420a98511bbf8f158dc6",
      "a668677a6f7247eaaded1e1d8c730d1f",
      "d376b59890b84bdd80bdc3a6d00ffebc",
      "52f1ba425f494f9a85b217aef0de9477",
      "cfcc1c3829e849ac8087ff394ca83479",
      "b897c77071224dad915e5a0a1d267d83",
      "1d0d97f879c5412e8ab6f900b2bd5231",
      "259572b65dfe4bddacc4801d4eb3ddba",
      "41ff5413811d43979bc05c3a9ef6cbff"
     ]
    },
    "id": "61e0F4ngUFiM",
    "outputId": "3122e954-2ebd-4cd4-ae16-64ddfdec9b34"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/5200 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7ea0c3bc1b1b46e1a2915aa98861b298"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SupervisedDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        prompt = self.dataset[idx][\"prompt\"]\n",
    "        output = self.dataset[idx][\"output\"]\n",
    "        full_prompt = prompt + output\n",
    "\n",
    "        tokenized = self.tokenizer(full_prompt, truncation=True, padding=\"max_length\", max_length=2048, return_tensors=\"pt\")\n",
    "        input_ids = tokenized.input_ids.squeeze(0)\n",
    "        labels = input_ids.clone()\n",
    "        return {\"input_ids\": input_ids, \"labels\": labels}\n",
    "\n",
    "train_dataset = SupervisedDataset(dataset, tokenizer)"
   ],
   "metadata": {
    "id": "v4AZksb6UFlI"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./tinyllama-finetuned\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    warmup_steps=10,\n",
    "    max_steps=100,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset_text_field=\"text\",\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510
    },
    "id": "uUnmywWEUFn_",
    "outputId": "4a9f5ccf-2d5f-4602-cf04-fe5ffb9c7324"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 5,200 | Num Epochs = 1 | Total steps = 100\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 1\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 1 x 1) = 1\n",
      " \"-____-\"     Trainable parameters = 4,505,600/4,000,000,000 (0.11% trained)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 01:35, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.315800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.567400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.295800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.117200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.251700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.972900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.025600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.007300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=1.2603971576690673, metrics={'train_runtime': 100.0285, 'train_samples_per_second': 1.0, 'train_steps_per_second': 1.0, 'total_flos': 1276745298739200.0, 'train_loss': 1.2603971576690673})"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model.save_pretrained(\"./tinyllama-finetuned-model\")\n",
    "tokenizer.save_pretrained(\"./tinyllama-finetuned-model\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fUVKnH9sUXji",
    "outputId": "2e628e1e-0690-4abd-9b6a-c13b88f0e8dc"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('./tinyllama-finetuned-model/tokenizer_config.json',\n",
       " './tinyllama-finetuned-model/special_tokens_map.json',\n",
       " './tinyllama-finetuned-model/tokenizer.model',\n",
       " './tinyllama-finetuned-model/added_tokens.json',\n",
       " './tinyllama-finetuned-model/tokenizer.json')"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def chat_response(prompt, max_new_tokens=200):\n",
    "    chat_prompt = f\"<|im_start|>user\\n{prompt}\\n<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    inputs = tokenizer(chat_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    start = decoded.find(\"<|im_start|>assistant\\n\")\n",
    "    if start != -1:\n",
    "        response = decoded[start + len(\"<|im_start|>assistant\\n\"):].strip()\n",
    "    else:\n",
    "        response = decoded.strip()\n",
    "    response = response.split(\"<|im_end|>\")[0].strip()\n",
    "    return response\n",
    "\n",
    "# Test the model\n",
    "test_prompts = [\n",
    "    \"Explain photosynthesis in simple terms.\",\n",
    "    \"What is the formula to find the area of a triangle?\",\n",
    "    \"Give me 5 tips to stay productive while working from home.\",\n",
    "    \"Write a short poem about the stars.\",\n",
    "    \"What is the meaning of life according to different philosophers?\",\n",
    "    \"How can I manage my time better as a student?\",\n",
    "    \"Explain the difference between RAM and ROM.\",\n",
    "    \"Suggest some daily habits to improve mental well-being.\",\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Response: {chat_response(prompt)}\")\n",
    "    print(\"=\"*100)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ui6dptIEUXmP",
    "outputId": "6aef05c4-21d4-4742-9b57-28aeab8ad9f7"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Prompt: Explain photosynthesis in simple terms.\n",
      "Response: Photosynthesis is the process by which plants, algae, and some bacteria convert sunlight into energy. It is a complex process that involves the conversion of carbon dioxide into glucose (a type of sugar) through the use of chlorophyll. The process is essential for the survival of many organisms, as it provides the energy needed for growth, reproduction, and survival.\n",
      "====================================================================================================\n",
      "Prompt: What is the formula to find the area of a triangle?\n",
      "Response: The formula to find the area of a triangle is A = 0.5(b x h).\n",
      "====================================================================================================\n",
      "Prompt: Give me 5 tips to stay productive while working from home.\n",
      "Response: 1. Stay organized: Use a planner or calendar to keep track of your tasks and deadlines.\n",
      "2. Set boundaries: Set specific times for work and personal time.\n",
      "3. Take breaks: Take regular breaks to stretch, walk, or do something else to avoid burnout.\n",
      "4. Stay connected: Use technology to stay connected with colleagues and clients.\n",
      "5. Prioritize tasks: Prioritize tasks based on their importance and urgency.\n",
      "====================================================================================================\n",
      "Prompt: Write a short poem about the stars.\n",
      "Response: The stars shine bright,\n",
      "A sight to behold,\n",
      "A sight that fills our hearts,\n",
      "With wonder and awe.\n",
      "====================================================================================================\n",
      "Prompt: What is the meaning of life according to different philosophers?\n",
      "Response: The meaning of life according to different philosophers is the question of what the purpose of life is. Different philosophers have different answers to this question, but they all agree that the purpose of life is to live a fulfilling and meaningful life.\n",
      "====================================================================================================\n",
      "Prompt: How can I manage my time better as a student?\n",
      "Response: As a student, it is essential to manage your time effectively. Here are some tips to help you do so:\n",
      "\n",
      "1. Prioritize your tasks: Make a list of your tasks and prioritize them based on their importance.\n",
      "\n",
      "2. Set realistic goals: Set achievable goals that are within your capabilities.\n",
      "\n",
      "3. Use time-management tools: Use time-management tools such as a planner, a timer, or a calendar to help you manage your time.\n",
      "\n",
      "4. Take breaks: Take breaks to recharge and avoid burnout.\n",
      "\n",
      "5. Prioritize your study time: Prioritize your study time based on the importance of the subject.\n",
      "\n",
      "6. Use time wisely: Use your time wisely by taking advantage of opportunities to learn and grow.\n",
      "\n",
      "7. Stay organized: Keep your workspace organized and avoid clutter.\n",
      "\n",
      "8. Be flexible: Be flexible with your schedule and\n",
      "====================================================================================================\n",
      "Prompt: Explain the difference between RAM and ROM.\n",
      "Response: RAM (Random Access Memory) is a type of memory that can be accessed quickly and is used to store data that is frequently accessed by the computer. ROM (Read Only Memory) is a type of memory that is used to store data that is not frequently accessed by the computer. ROM is used to store the basic instructions and data that the computer needs to run, while RAM is used to store data that is frequently accessed by the computer.\n",
      "====================================================================================================\n",
      "Prompt: Suggest some daily habits to improve mental well-being.\n",
      "Response: Daily habits to improve mental well-being include:\n",
      "1. Practice mindfulness meditation\n",
      "2. Exercise regularly\n",
      "3. Get enough sleep\n",
      "4. Eat a healthy diet\n",
      "5. Stay connected with friends and family\n",
      "6. Practice gratitude\n",
      "7. Take breaks throughout the day\n",
      "8. Practice self-care activities such as massage or acupuncture.\n",
      "====================================================================================================\n"
     ]
    }
   ]
  }
 ]
}